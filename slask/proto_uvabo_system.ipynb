{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f366b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62d930",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4178f07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.799167</td>\n",
       "      <td>1.605204</td>\n",
       "      <td>1.995852</td>\n",
       "      <td>1.166502</td>\n",
       "      <td>0.887902</td>\n",
       "      <td>0.439629</td>\n",
       "      <td>1.053714</td>\n",
       "      <td>1.045603</td>\n",
       "      <td>0.633024</td>\n",
       "      <td>0.781121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436555</td>\n",
       "      <td>0.938103</td>\n",
       "      <td>1.231281</td>\n",
       "      <td>0.977423</td>\n",
       "      <td>0.762809</td>\n",
       "      <td>1.657038</td>\n",
       "      <td>0.794413</td>\n",
       "      <td>1.339076</td>\n",
       "      <td>0.571211</td>\n",
       "      <td>1.762915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.874654</td>\n",
       "      <td>1.519406</td>\n",
       "      <td>2.662668</td>\n",
       "      <td>1.388321</td>\n",
       "      <td>2.054822</td>\n",
       "      <td>1.153831</td>\n",
       "      <td>0.613322</td>\n",
       "      <td>0.847776</td>\n",
       "      <td>0.545768</td>\n",
       "      <td>1.445538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685636</td>\n",
       "      <td>1.085547</td>\n",
       "      <td>0.824725</td>\n",
       "      <td>1.665812</td>\n",
       "      <td>1.456734</td>\n",
       "      <td>1.027148</td>\n",
       "      <td>1.105278</td>\n",
       "      <td>1.551257</td>\n",
       "      <td>0.813239</td>\n",
       "      <td>2.092641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.858839</td>\n",
       "      <td>1.611884</td>\n",
       "      <td>1.137961</td>\n",
       "      <td>1.333731</td>\n",
       "      <td>1.068095</td>\n",
       "      <td>0.677794</td>\n",
       "      <td>1.018605</td>\n",
       "      <td>0.626380</td>\n",
       "      <td>0.604631</td>\n",
       "      <td>1.114316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322676</td>\n",
       "      <td>0.859049</td>\n",
       "      <td>0.998932</td>\n",
       "      <td>1.355352</td>\n",
       "      <td>0.732222</td>\n",
       "      <td>1.231596</td>\n",
       "      <td>0.665590</td>\n",
       "      <td>1.084211</td>\n",
       "      <td>0.771778</td>\n",
       "      <td>1.564389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.069992</td>\n",
       "      <td>1.668252</td>\n",
       "      <td>1.413955</td>\n",
       "      <td>1.586725</td>\n",
       "      <td>1.348607</td>\n",
       "      <td>1.196058</td>\n",
       "      <td>0.090398</td>\n",
       "      <td>2.157366</td>\n",
       "      <td>2.356820</td>\n",
       "      <td>1.593439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827649</td>\n",
       "      <td>1.029577</td>\n",
       "      <td>0.907566</td>\n",
       "      <td>1.791083</td>\n",
       "      <td>1.370967</td>\n",
       "      <td>1.035722</td>\n",
       "      <td>1.180668</td>\n",
       "      <td>1.537546</td>\n",
       "      <td>0.915133</td>\n",
       "      <td>1.623220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.985379</td>\n",
       "      <td>1.642372</td>\n",
       "      <td>1.544812</td>\n",
       "      <td>1.402642</td>\n",
       "      <td>1.160583</td>\n",
       "      <td>0.783853</td>\n",
       "      <td>1.016836</td>\n",
       "      <td>0.552110</td>\n",
       "      <td>0.581350</td>\n",
       "      <td>1.205812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488827</td>\n",
       "      <td>0.760080</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>1.574881</td>\n",
       "      <td>1.050606</td>\n",
       "      <td>1.294741</td>\n",
       "      <td>0.857752</td>\n",
       "      <td>1.243329</td>\n",
       "      <td>0.979571</td>\n",
       "      <td>1.452971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.543035</td>\n",
       "      <td>1.027566</td>\n",
       "      <td>1.547426</td>\n",
       "      <td>0.964821</td>\n",
       "      <td>1.304017</td>\n",
       "      <td>0.845071</td>\n",
       "      <td>0.736679</td>\n",
       "      <td>0.915263</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>1.012005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724051</td>\n",
       "      <td>1.197753</td>\n",
       "      <td>1.328963</td>\n",
       "      <td>1.182362</td>\n",
       "      <td>1.227438</td>\n",
       "      <td>1.711086</td>\n",
       "      <td>1.120406</td>\n",
       "      <td>2.184267</td>\n",
       "      <td>0.771658</td>\n",
       "      <td>0.435770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.021482</td>\n",
       "      <td>1.591179</td>\n",
       "      <td>1.600068</td>\n",
       "      <td>1.301523</td>\n",
       "      <td>0.590889</td>\n",
       "      <td>0.684798</td>\n",
       "      <td>1.030120</td>\n",
       "      <td>0.177209</td>\n",
       "      <td>0.641048</td>\n",
       "      <td>2.800425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668547</td>\n",
       "      <td>1.814397</td>\n",
       "      <td>1.210936</td>\n",
       "      <td>1.579076</td>\n",
       "      <td>1.163361</td>\n",
       "      <td>1.399544</td>\n",
       "      <td>0.865772</td>\n",
       "      <td>1.179742</td>\n",
       "      <td>1.107659</td>\n",
       "      <td>1.303708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.973127</td>\n",
       "      <td>1.583572</td>\n",
       "      <td>1.724784</td>\n",
       "      <td>1.636147</td>\n",
       "      <td>1.283576</td>\n",
       "      <td>1.299140</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.888346</td>\n",
       "      <td>0.741038</td>\n",
       "      <td>1.649702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850389</td>\n",
       "      <td>0.989390</td>\n",
       "      <td>0.916578</td>\n",
       "      <td>1.816426</td>\n",
       "      <td>1.493339</td>\n",
       "      <td>1.087889</td>\n",
       "      <td>1.257658</td>\n",
       "      <td>1.684142</td>\n",
       "      <td>1.151303</td>\n",
       "      <td>1.957084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.355253</td>\n",
       "      <td>0.913831</td>\n",
       "      <td>1.498294</td>\n",
       "      <td>1.041697</td>\n",
       "      <td>0.814316</td>\n",
       "      <td>1.036855</td>\n",
       "      <td>0.622521</td>\n",
       "      <td>1.009190</td>\n",
       "      <td>0.374934</td>\n",
       "      <td>1.235581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775802</td>\n",
       "      <td>0.390989</td>\n",
       "      <td>1.161164</td>\n",
       "      <td>1.415946</td>\n",
       "      <td>1.212896</td>\n",
       "      <td>1.529771</td>\n",
       "      <td>1.164924</td>\n",
       "      <td>1.562210</td>\n",
       "      <td>0.555465</td>\n",
       "      <td>1.896242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.854961</td>\n",
       "      <td>1.596457</td>\n",
       "      <td>1.544308</td>\n",
       "      <td>1.594522</td>\n",
       "      <td>2.843717</td>\n",
       "      <td>1.264706</td>\n",
       "      <td>0.919485</td>\n",
       "      <td>0.988844</td>\n",
       "      <td>0.815698</td>\n",
       "      <td>1.665082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910386</td>\n",
       "      <td>0.051707</td>\n",
       "      <td>0.970749</td>\n",
       "      <td>1.731640</td>\n",
       "      <td>1.516886</td>\n",
       "      <td>1.024586</td>\n",
       "      <td>1.223308</td>\n",
       "      <td>1.723625</td>\n",
       "      <td>0.947748</td>\n",
       "      <td>2.083739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.600350</td>\n",
       "      <td>0.857435</td>\n",
       "      <td>1.615126</td>\n",
       "      <td>1.159307</td>\n",
       "      <td>0.755840</td>\n",
       "      <td>1.017698</td>\n",
       "      <td>0.573636</td>\n",
       "      <td>0.869458</td>\n",
       "      <td>0.346960</td>\n",
       "      <td>1.309627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863172</td>\n",
       "      <td>0.965042</td>\n",
       "      <td>1.286095</td>\n",
       "      <td>1.472389</td>\n",
       "      <td>1.363714</td>\n",
       "      <td>1.548470</td>\n",
       "      <td>1.263240</td>\n",
       "      <td>1.722842</td>\n",
       "      <td>0.837297</td>\n",
       "      <td>1.805512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.867188</td>\n",
       "      <td>1.609924</td>\n",
       "      <td>0.885528</td>\n",
       "      <td>1.242892</td>\n",
       "      <td>1.095870</td>\n",
       "      <td>0.993149</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>1.055402</td>\n",
       "      <td>2.632691</td>\n",
       "      <td>1.245716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581432</td>\n",
       "      <td>1.127512</td>\n",
       "      <td>1.042018</td>\n",
       "      <td>1.416286</td>\n",
       "      <td>1.015452</td>\n",
       "      <td>1.390430</td>\n",
       "      <td>0.910582</td>\n",
       "      <td>1.857888</td>\n",
       "      <td>0.428646</td>\n",
       "      <td>2.025226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.799167  1.605204  1.995852  1.166502  0.887902  0.439629  1.053714   \n",
       "1   0.874654  1.519406  2.662668  1.388321  2.054822  1.153831  0.613322   \n",
       "2   0.858839  1.611884  1.137961  1.333731  1.068095  0.677794  1.018605   \n",
       "3   1.069992  1.668252  1.413955  1.586725  1.348607  1.196058  0.090398   \n",
       "4   0.985379  1.642372  1.544812  1.402642  1.160583  0.783853  1.016836   \n",
       "5   0.543035  1.027566  1.547426  0.964821  1.304017  0.845071  0.736679   \n",
       "6   1.021482  1.591179  1.600068  1.301523  0.590889  0.684798  1.030120   \n",
       "7   0.973127  1.583572  1.724784  1.636147  1.283576  1.299140  0.812604   \n",
       "8   0.355253  0.913831  1.498294  1.041697  0.814316  1.036855  0.622521   \n",
       "9   1.854961  1.596457  1.544308  1.594522  2.843717  1.264706  0.919485   \n",
       "10  0.600350  0.857435  1.615126  1.159307  0.755840  1.017698  0.573636   \n",
       "11  0.867188  1.609924  0.885528  1.242892  1.095870  0.993149  0.974603   \n",
       "\n",
       "          7         8         9   ...        30        31        32        33  \\\n",
       "0   1.045603  0.633024  0.781121  ...  0.436555  0.938103  1.231281  0.977423   \n",
       "1   0.847776  0.545768  1.445538  ...  0.685636  1.085547  0.824725  1.665812   \n",
       "2   0.626380  0.604631  1.114316  ...  0.322676  0.859049  0.998932  1.355352   \n",
       "3   2.157366  2.356820  1.593439  ...  0.827649  1.029577  0.907566  1.791083   \n",
       "4   0.552110  0.581350  1.205812  ...  0.488827  0.760080  0.948980  1.574881   \n",
       "5   0.915263  0.373700  1.012005  ...  0.724051  1.197753  1.328963  1.182362   \n",
       "6   0.177209  0.641048  2.800425  ...  0.668547  1.814397  1.210936  1.579076   \n",
       "7   0.888346  0.741038  1.649702  ...  0.850389  0.989390  0.916578  1.816426   \n",
       "8   1.009190  0.374934  1.235581  ...  0.775802  0.390989  1.161164  1.415946   \n",
       "9   0.988844  0.815698  1.665082  ...  0.910386  0.051707  0.970749  1.731640   \n",
       "10  0.869458  0.346960  1.309627  ...  0.863172  0.965042  1.286095  1.472389   \n",
       "11  1.055402  2.632691  1.245716  ...  0.581432  1.127512  1.042018  1.416286   \n",
       "\n",
       "          34        35        36        37        38        39  \n",
       "0   0.762809  1.657038  0.794413  1.339076  0.571211  1.762915  \n",
       "1   1.456734  1.027148  1.105278  1.551257  0.813239  2.092641  \n",
       "2   0.732222  1.231596  0.665590  1.084211  0.771778  1.564389  \n",
       "3   1.370967  1.035722  1.180668  1.537546  0.915133  1.623220  \n",
       "4   1.050606  1.294741  0.857752  1.243329  0.979571  1.452971  \n",
       "5   1.227438  1.711086  1.120406  2.184267  0.771658  0.435770  \n",
       "6   1.163361  1.399544  0.865772  1.179742  1.107659  1.303708  \n",
       "7   1.493339  1.087889  1.257658  1.684142  1.151303  1.957084  \n",
       "8   1.212896  1.529771  1.164924  1.562210  0.555465  1.896242  \n",
       "9   1.516886  1.024586  1.223308  1.723625  0.947748  2.083739  \n",
       "10  1.363714  1.548470  1.263240  1.722842  0.837297  1.805512  \n",
       "11  1.015452  1.390430  0.910582  1.857888  0.428646  2.025226  \n",
       "\n",
       "[12 rows x 40 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synthetic problem\n",
    "r = np.random.random((12,3))\n",
    "s = np.random.random((40,3))\n",
    "\n",
    "q = np.sqrt(np.sum((np.expand_dims(r,1) - np.expand_dims(s,0))**2,axis=2))\n",
    "offsets_gt = np.random.rand(1,q.shape[1])\n",
    "q = q + offsets_gt + 0.05*np.random.randn(*q.shape)\n",
    "outlier_idx = np.random.rand(*q.shape) < 0.1\n",
    "q[outlier_idx] = np.random.rand(*q.shape)[outlier_idx]*3\n",
    "\n",
    "tdoa_df = pd.DataFrame(q)\n",
    "tdoa = tdoa_df.to_numpy()\n",
    "tdoa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b33525",
   "metadata": {},
   "source": [
    "# Helper Functions ans Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a134ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UvaboSolution():\n",
    "\n",
    "    def __init__(self, n_receivers, n_senders, dim=3):\n",
    "        u = np.empty((3, n_receivers))\n",
    "        u[:] = np.nan\n",
    "        self.u = u\n",
    "\n",
    "        v = np.empty((3, n_senders))\n",
    "        v[:] = np.nan\n",
    "        self.v = v\n",
    "\n",
    "        a = np.empty((n_receivers,1))\n",
    "        a[:] = np.nan\n",
    "        self.a = a\n",
    "\n",
    "        b = np.empty((1, n_senders))\n",
    "        b[:] = np.nan\n",
    "        self.b = b\n",
    "\n",
    "        o = np.empty((1,n_senders))\n",
    "        o[:] = np.nan\n",
    "        self.o = o\n",
    "\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "def init_uvab(d):\n",
    "    dsquare = d**2\n",
    "    doubleCompaction = dsquare - dsquare[0:1,:] - dsquare[:,0:1] + dsquare[0,0]\n",
    "    uu,ss,vv = np.linalg.svd(doubleCompaction/-2)\n",
    "    u =uu[:,:3].T\n",
    "    v = np.diag(ss[:3])@vv[:3,:]\n",
    "    a = dsquare[:,0:1] - dsquare[0,0]/2\n",
    "    b = dsquare[0:1,:] - dsquare[0,0]/2\n",
    "    return u,v,a,b\n",
    "\n",
    "class TxoaType(Enum):\n",
    "    TOA = 1\n",
    "    TDOA = 2\n",
    "    COTDOA = 3\n",
    "\n",
    "class TxoaProblem():\n",
    "    def __init__(self, data, problem_type=TxoaType.TDOA, dim=3):\n",
    "        self.data = data\n",
    "        self.sol = UvaboSolution(*data.shape, dim=dim)\n",
    "        self.problem_type = problem_type\n",
    "        self.dim = dim\n",
    "\n",
    "    def solve_for_offset(self, solver, tol=0.01):   \n",
    "        #sample_subset = lambda : self.data[np.random.permutation(self.data.shape[0])[:solver.get_needed_receivers()],np.random.permutation(self.data.shape[1])[:solver.get_needed_senders()]]\n",
    "\n",
    "        outer_ransac_iters = 100\n",
    "        most_inliers = -1\n",
    "        best_sol = None\n",
    "        for _ in range(outer_ransac_iters):\n",
    "            mics_choice = np.random.permutation(self.data.shape[0])[:solver.get_needed_receivers()]\n",
    "            sound_choice = np.random.permutation(self.data.shape[1])[:solver.get_needed_senders()]\n",
    "            data = self.data[mics_choice][:,sound_choice]\n",
    "\n",
    "            offsets = solver.solve(data)\n",
    "            d = data - offsets\n",
    "\n",
    "            u,v,a,b = init_uvab(d)\n",
    "\n",
    "            cur_solution = UvaboSolution(*self.data.shape,dim=self.dim)\n",
    "\n",
    "            cur_solution.u[:,mics_choice] = u\n",
    "            cur_solution.v[:,sound_choice] = v\n",
    "            cur_solution.a[mics_choice] = a\n",
    "            cur_solution.b[:,sound_choice] = b\n",
    "            cur_solution.o[:,sound_choice] = offsets\n",
    "            cur_problem = TxoaProblem(self.data, problem_type=self.problem_type, dim=self.dim)\n",
    "            cur_problem.sol = cur_solution\n",
    "\n",
    "            cur_problem.ransac_expand_to_all_cols()\n",
    "\n",
    "            res = -2*cur_solution.u.T@cur_solution.v + cur_solution.a + cur_solution.b - (self.data - cur_solution.o)**2\n",
    "\n",
    "            if np.sum(np.abs(res) < tol) > most_inliers:\n",
    "                most_inliers = np.sum(np.abs(res) < tol)\n",
    "                best_sol = cur_solution\n",
    "        self.sol = best_sol\n",
    "\n",
    "    def ransac_expand_col(self, new_sound_idx, ransac_iter=10,tol=0.01):\n",
    "        needed_eqs = self.dim + 2\n",
    "        most_inliers = -1\n",
    "        best_sol = None\n",
    "        known_mics = np.argwhere(np.logical_not(np.isnan(self.sol.a)))[:,0]\n",
    "\n",
    "        for _ in range(ransac_iter):\n",
    "            mic_local_choice = np.random.permutation(known_mics.shape[0])[:needed_eqs]\n",
    "            M = np.concatenate([2*self.data[known_mics[mic_local_choice],new_sound_idx:new_sound_idx+1],\n",
    "                                -np.ones((needed_eqs,1)),\n",
    "                                -2*self.sol.u[:,known_mics[mic_local_choice]].T,\n",
    "                                np.ones((needed_eqs,1))\n",
    "                                ],axis=1) # variable order is [o_j, o_j^2, v_j, b_j]\n",
    "            B = self.data[known_mics[mic_local_choice],new_sound_idx:new_sound_idx+1]**2 - self.sol.a[known_mics[mic_local_choice]]\n",
    "            if np.any(np.isnan(M)) or np.any(np.isnan(B)):\n",
    "                print(\"O no! found none\")\n",
    "                continue\n",
    "            x_part,_,_,_ = np.linalg.lstsq(M,B,rcond=None)\n",
    "            x_hom = np.array([0,1,*([0]*self.dim),1])\n",
    "            x_hom = np.expand_dims(x_hom,1)\n",
    "            res = x_part[0]**2 - x_part[1]\n",
    "            x = x_hom*res + x_part \n",
    "\n",
    "            o_new = x[0]\n",
    "            v_new = x[2:5]\n",
    "            b_new = x[5]\n",
    "\n",
    "            lh = np.expand_dims((self.data[known_mics,new_sound_idx] - o_new)**2,axis=1)\n",
    "            rh = -2*self.sol.u[:,known_mics].T@v_new+self.sol.a[known_mics]+b_new\n",
    "\n",
    "            res = lh - rh\n",
    "            if np.sum(np.abs(res) < tol) > most_inliers:\n",
    "                most_inliers = np.sum(np.abs(res) < tol)\n",
    "                best_sol = (o_new[0], v_new[:,0], b_new[0])\n",
    "        self.sol.o[0,new_sound_idx] = best_sol[0]\n",
    "        self.sol.v[:,new_sound_idx] = best_sol[1]\n",
    "        self.sol.b[0,new_sound_idx] = best_sol[2]\n",
    "\n",
    "    def ransac_expand_row(self, new_mic_idx, ransac_iter=10,tol=0.01):\n",
    "        \n",
    "        known_sounds = np.argwhere(np.logical_not(np.isnan(self.sol.b)))[:,1]\n",
    "        needed_eqs = self.dim + 1\n",
    "        most_inliers = -1\n",
    "        best_sol = None\n",
    "        for _ in range(ransac_iter):\n",
    "            sound_choice = known_sounds[np.random.permutation(known_sounds.shape[0])[:needed_eqs]]\n",
    "            M = np.concatenate([-2*self.sol.v[:,sound_choice].T,\n",
    "                                np.ones((needed_eqs,1))\n",
    "                                ],axis=1) # variable order is [o_j, o_j^2, v_j, b_j]\n",
    "            B = (self.data[new_mic_idx,sound_choice] - self.sol.o[0,sound_choice])**2 - self.sol.b[0,sound_choice]\n",
    "            #B = tdoa[mics_choice[mic_local_choice],new_sound_idx:new_sound_idx+1]**2 - a[mic_local_choice] \n",
    "            if np.any(np.isnan(M)) or np.any(np.isnan(B)):\n",
    "                print(\"O no! found none\")\n",
    "                continue\n",
    "            x = np.linalg.solve(M,B)\n",
    "\n",
    "            u_new = np.expand_dims(x[:3],axis=1)\n",
    "            a_new = x[3]\n",
    "\n",
    "            lh = np.expand_dims((self.data[new_mic_idx,known_sounds] - self.sol.o[:,known_sounds])**2,axis=1)\n",
    "            rh = -2*u_new.T@self.sol.v[:,known_sounds]+a_new+self.sol.b[:,known_sounds]\n",
    "\n",
    "            res = lh - rh\n",
    "            if np.sum(np.abs(res) < tol) > most_inliers:\n",
    "                most_inliers = np.sum(np.abs(res) < tol)\n",
    "                best_sol = (u_new, a_new)\n",
    "        self.sol.u[:,new_mic_idx] = best_sol[0][:,0]\n",
    "        self.sol.a[new_mic_idx] = best_sol[1]\n",
    "\n",
    "    def ransac_expand_to_all_cols(self, ransac_iter=10, tol=0.01):\n",
    "        known_sounds = np.argwhere(np.logical_not(np.isnan(self.sol.b)))[:,1]\n",
    "        for new_sound in np.setdiff1d(np.arange(self.data.shape[1]),known_sounds):\n",
    "            self.ransac_expand_col(new_sound,ransac_iter=ransac_iter, tol=tol)\n",
    "    \n",
    "    def ransac_expand_to_all_rows(self, ransac_iter=10, tol=0.01):\n",
    "        known_mics = np.argwhere(np.logical_not(np.isnan(self.sol.a)))[:,0]\n",
    "        for new_mic in np.setdiff1d(np.arange(self.data.shape[0]),known_mics):\n",
    "            self.ransac_expand_row(new_mic,ransac_iter=ransac_iter, tol=tol)\n",
    "        \n",
    "    def get_residuals(self):\n",
    "        return (-2*self.sol.u.T@self.sol.v + self.sol.a + self.sol.b) - (self.data - self.sol.o)**2\n",
    "\n",
    "    def bundle(self, lr=3e-3, steps=30, tol=0.1):\n",
    "        dtype = torch.float32\n",
    "        good_rows = np.logical_not(np.isnan(self.sol.a[:,0]))\n",
    "        good_cols = np.logical_not(np.isnan(self.sol.b[0]))\n",
    "        u = nn.Parameter(torch.tensor(self.sol.u[:,good_rows],dtype=dtype))\n",
    "        v = nn.Parameter(torch.tensor(self.sol.v[:,good_cols],dtype=dtype))\n",
    "        a = nn.Parameter(torch.tensor(self.sol.a[good_rows],dtype=dtype))\n",
    "        b = nn.Parameter(torch.tensor(self.sol.b[:,good_cols],dtype=dtype))\n",
    "        o = nn.Parameter(torch.tensor(self.sol.o[:,good_cols],dtype=dtype))\n",
    "        data = torch.tensor(self.data[good_rows][:,good_cols],dtype=dtype)\n",
    "        optimizer = torch.optim.Adam([u,v,a,b,o],lr=lr)\n",
    "\n",
    "        #compute_estimate = lambda u,v,a,b,o : (-2*u.T@v + a + b)**0.5 + o # Had problem with nan values spreading when using this loss space\n",
    "        compute_part_estimate = lambda u,v,a,b : (-2*u.T@v + a + b)\n",
    "        \n",
    "        huberloss = torch.nn.HuberLoss(delta=tol)\n",
    "        for _ in range(steps):\n",
    "\n",
    "            est = compute_part_estimate(u,v,a,b)\n",
    "            good_idx = (est ** 0.5 + o).isnan().logical_not()\n",
    "\n",
    "            loss = huberloss(est[good_idx],((data - o)**2)[good_idx]) \n",
    "            loss += tol*torch.maximum(torch.tensor(0),-est[est.isnan().logical_not()]).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.sol.u[:,good_rows] = u.detach().numpy().astype(dtype=self.sol.u.dtype)\n",
    "        self.sol.v[:,good_cols] = v.detach().numpy().astype(dtype=self.sol.v.dtype)\n",
    "        self.sol.a[good_rows] = a.detach().numpy().astype(dtype=self.sol.a.dtype)\n",
    "        self.sol.b[:,good_cols] = b.detach().numpy().astype(dtype=self.sol.b.dtype)\n",
    "        self.sol.o[:,good_cols] = o.detach().numpy().astype(dtype=self.sol.o.dtype)\n",
    "\n",
    "class OffsetSolver(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_needed_receivers():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_needed_senders():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(data):\n",
    "        pass\n",
    "    \n",
    "class OffsetSolver95(OffsetSolver):\n",
    "\n",
    "    def get_needed_receivers():\n",
    "        return 9\n",
    "    \n",
    "    def get_needed_senders():\n",
    "        return 5\n",
    "    \n",
    "    def solve(data):\n",
    "        zsquared = data ** 2\n",
    "        A = np.concatenate([zsquared[:,1:] - zsquared[:,0:1], -2*data[:,1:], 2*data[:,0:1]],axis=1)\n",
    "        u = np.linalg.solve(A, np.ones(9))\n",
    "        sols = np.concatenate([u[-1:]/np.sum(u[:4]),u[4:-1]/u[:4]],axis=0)\n",
    "        return sols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55fec6",
   "metadata": {},
   "source": [
    "# Run System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebee88dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp = TxoaProblem(tdoa)\n",
    "tp.solve_for_offset(OffsetSolver95)\n",
    "tp.bundle(steps=100)\n",
    "tp.ransac_expand_to_all_rows()\n",
    "tp.bundle(steps=100)\n",
    "plt.hist(tp.get_residuals().flatten(),np.arange(-3,3,0.05));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
